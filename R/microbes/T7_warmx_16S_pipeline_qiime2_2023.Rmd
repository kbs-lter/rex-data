---
title: "2023 16S Pipeline"
author: "Moriah Young"
date: "2024-04-29"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{bash}
#load anaconda
module load Conda/3

#activate qiime2
conda activate qiime2-2022.11

# random notes
# Ctrl + C to stop a process running on a terminal
#SBATCH --output=~/%x-%j.slurmout # x is job name and j is job number and the outputs will go here
```

Import demultiplexed sequences into .qza artifact for downstream pipeline using Qiime2
```{bash}
cd /mnt/home/youngmor/20240415_16SV4_PE250
qiime tools import 
--type 'SampleData[PairedEndSequencesWithQuality]' \
--input-path 20240415_16SV4_PE250_manifest.txt/ \
--input-format PairedEndFastqManifestPhred33V2 \
--output-path 20240415_16SV4_PE250.qza \

# enter the code below in one string like this to run on the hpcc
qiime tools import --type 'SampleData[PairedEndSequencesWithQuality]' --input-path 20240415_16SV4_PE250_manifest.txt --input-format PairedEndFastqManifestPhred33V2 --output-path 20240415_16SV4_PE250.qza

````

Denoise sequences using dada2
```{bash}
qiime demux summarize \
--i-data 20240415_16SV4_PE250.qza \
--o-visualization 20240415_16SV4_PE250_PE250_SUMMARY_VIZ.qzv
qiime tools view 20240415_16SV4_PE250_PE250_SUMMARY_VIZ.qzv # https://view.qiime2.org/

qiime demux summarize --i-data 20240415_16SV4_PE250.qza --o-visualization 20240415_16SV4_PE250_PE250_SUMMARY_VIZ.qzv
```

Create a .qzv file so you can check out the table from the denoising stats that was created from the job above
You can then drag and drop the .qzv file into this website: https://view.qiime2.org/ 
You'll need to download it from the interactive interface onto your mac
```{bash}
# Run 1
cd /mnt/home/youngmor/20240415_16SV4_PE250
qiime metadata tabulate \
  --m-input-file 20240415_16SV4_PE250-denoising-stats.qza \
  --o-visualization 20240415_16SV4_PE250-denoising-stats.qzv
  
qiime metadata tabulate --m-input-file 20240415_16SV4_PE250-denoising-stats.qza --o-visualization 20240415_16SV4_PE250-denoising-stats.qzv
```

# Training a classifier
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5069754/ # primers
```{bash}
cd /mnt/home/youngmor/20240415_16SV4_PE250
# test the classifier
qiime feature-classifier classify-sklearn \
  --i-classifier classifier.qza \
  --i-reads 20240415_16SV4_PE250-rep-seqs.qza \
  --o-classification 16S-taxonomy-2023.qza

qiime feature-classifier classify-sklearn --i-classifier classifier.qza --i-reads 20240415_16SV4_PE250-rep-seqs.qza --o-classification 16S-taxonomy-2023.qza

# visualize
qiime metadata tabulate \
  --m-input-file 16S-taxonomy-2023.qza \
  --o-visualization 16S-taxonomy-2023.qzv

# testing the classifier and creating the .qzv file took 7 hours on the hpcc (using the same SLURM code/settings as the denoising script above)
  
qiime metadata tabulate --m-input-file 16S-taxonomy-2023.qza --o-visualization 16S-taxonomy-2023.qzv
```

# Generate a phylogenetic tree
https://docs.qiime2.org/2023.2/tutorials/phylogeny/#root-the-phylogeny
```{bash}
cd /mnt/home/youngmor/20240415_16SV4_PE250/

# this aligns your sequences
qiime alignment mafft \
  --i-sequences 16S-2023-rep-seqs.qza \
  --o-alignment 16S-2023-aligned-rep-seqs.qza
  
qiime alignment mafft --i-sequences 16S-2023-rep-seqs.qza --o-alignment 16S-2023-aligned-rep-seqs.qza
  
qiime alignment mask \
  --i-alignment 16S-2023-aligned-rep-seqs.qza \
  --o-masked-alignment 16S-2023-masked-aligned-rep-seqs.qza
  
qiime alignment mask --i-alignment 16S-2023-aligned-rep-seqs.qza --o-masked-alignment 16S-2023-masked-aligned-rep-seqs.qza

# make the tree
# using fasttree
qiime phylogeny fasttree \
  --i-alignment 16S-2023-masked-aligned-rep-seqs.qza \
  --o-tree 16S-2023-fasttree-unrooted-tree.qza
  --verbose
  
qiime phylogeny fasttree --i-alignment 16S-2023-masked-aligned-rep-seqs.qza --o-tree 16S-2023-fasttree-tree.qza --verbose

#root the tree using the longest root
qiime phylogeny midpoint-root \
  --i-tree 16S-2023-fasttree-unrooted-tree.qza \
  --o-rooted-tree 16S-2023-fasttree-rooted-tree.qza

qiime phylogeny midpoint-root --i-tree 16S-2023-fasttree-unrooted-tree.qza --o-rooted-tree 16S-2023-fasttree-rooted-tree.qza


# code below worth trying in the future - I think it's supposed to do all the steps above
------------
qiime phylogeny align-to-tree-mafft-fasttree \
  --i-sequences rep-seqs.qza \
  --output-dir mafft-fasttree-output
-------------
```

# create mapping/metadata file
```{r}
# Clear all existing data
rm(list=ls())

# Set working directory from .Renviron
dir <- Sys.getenv("DATA_DIR")
list.files(dir)

library(tidyverse)

# read in data
microbe_metadata <- read.csv(file.path(dir, "microbes/16S & ITS/T7_warmx/L0/2023/files to create REX 16S 2023 metadata/MY_REX_soil_microbes_2023_metadata.csv"))
warmx_metadata <- read.csv(file.path(dir, "microbes/16S & ITS/T7_warmx/L0/2023/files to create REX 16S 2023 metadata/REX_warmx_metadata.csv"))

# rename columns to match each other
names(microbe_metadata)[names(microbe_metadata) == "Plot_ID"] <- "Unique_ID"
names(microbe_metadata)[names(microbe_metadata) == "FP_location"] <- "Footprint_Location"
names(microbe_metadata)[names(microbe_metadata) == "Subplot_location"] <- "Subplot_Location"
names(microbe_metadata)[names(microbe_metadata) == "Subplot_ID"] <- "Sample_ID"

# merge files above
bacterial_2023_metadata <- full_join(microbe_metadata, warmx_metadata, by = c("Unique_ID", "Treatment", "Replicate",
                                      "Footprint", "Footprint_Location", "Subplot", "Subplot_Location"))

# delete columns
# Unique_Field_Location_Code
bacterial_2023_metadata <- subset(bacterial_2023_metadata, select = -c(Unique_Field_Location_Code))

# reorder columns
bacterial_2023_metadata <- bacterial_2023_metadata[, c("Sample_ID", "Treatment", "Replicate", "Rep", "Footprint",
                                                       "Footprint_Location", "Rep_Footprint", "Footprint_Treatment_full",
                                                       "Subplot", "Subplot_Location", "Drought", "Warming",
                                                       "Insecticide", "Subplot_Descriptions",
                                                       "Unique_ID", "sampling_period")]

seq_metadata <- read.csv(file.path(dir, "microbes/16S & ITS/T7_warmx/L0/2023/files to create REX 16S 2023 metadata/sequence_2023_metadata.csv"))

# rename columns to match each other
names(seq_metadata)[names(seq_metadata) == "Sample_ID"] <- "RTSF_SampleID"
names(seq_metadata)[names(seq_metadata) == "Sample.Name"] <- "Sample_ID"

bacterial_2023_metadata <- full_join(seq_metadata, bacterial_2023_metadata, by = "Sample_ID")

# reorder columns
bacterial_2023_metadata <- bacterial_2023_metadata[, c("Sample_ID", "RTSF_SampleID", "Barcode",
                                                       "LinkerPrimerSequence", "Reverse_Primer", 
                                                       "MiSeqRun", "Treatment", "Replicate", 
                                                       "Rep", "Footprint", "Footprint_Location", 
                                                       "Rep_Footprint", "Footprint_Treatment_full",
                                                       "Subplot", "Subplot_Location", "Drought", "Warming",
                                                       "Insecticide", "Subplot_Descriptions",
                                                       "Unique_ID", "sampling_period", "Sample_or_Control")]

colnames(bacterial_2023_metadata)[1] <- "sampleid"

# removing rows that aren't control samples or true samples
bacterial_2023_metadata <- bacterial_2023_metadata[-c(236,241),]

# get rid of controls
#bacterial_2021_metadata <- bacterial_2021_metadata[!grepl("CONTROL", bacterial_2021_metadata$sampleid),]
#bacterial_2021_metadata <- bacterial_2021_metadata %>% na.omit() # get rid of line 227 which is sample 36 (sample not submitted to RTSF)

# write a new csv with the cleaned and merge data 
write.csv(bacterial_2023_metadata, file = "/Users/moriahyoung/Desktop/phyloseq_object/bacterial_2023_metadata.csv", row.names = F)
#write.table(bacterial_2021_metadata, file = "/Users/moriahyoung/Downloads/bacterial_2021_metadata.tsv", quote = F, sep = #",", row.names = F)
```

Cleaning and steps to make a phyloseq object
```{r}
library(qiime2R)
# Read QZA files into dataframe, re-format taxonomic tables, and re-upload them as .csv files
# Code below you only need to do once
SVs16S <- read_qza("/Users/moriahyoung/Downloads/16S-2023-dada2table.qza")
SVs16Stable <- SVs16S$data
write.csv(SVs16Stable, file = "/Users/moriahyoung/Downloads/16S-2023-dada2table.csv")

taxonomy16S <- read_qza("/Users/moriahyoung/Downloads/16S-2023-taxonomy.qza")
tax16S <- taxonomy16S$data %>% as_tibble() %>%
  mutate(Taxon=gsub("[a-z]__", "", Taxon)) %>% 
  separate(Taxon, sep=";", c("Kingdom","Phylum","Class","Order","Family","Genus","Species"))%>%
  mutate(Phylum=replace_na(Phylum,"empty"))
write.csv(tax16S, file = "/Users/moriahyoung/Downloads/16S-2023-taxonomy.csv", row.names =F)
```

Filter data
```{r}
library(phyloseq)
library(microbiome)
# create phyloseq object
data_16S_unfiltered <- read_csv2phyloseq(otu.file = "/Users/moriahyoung/Downloads/16S-2023-dada2table.csv",
                                         taxonomy.file = "/Users/moriahyoung/Downloads/16S-2023-taxonomy.csv",
                                         metadata.file = "/Users/moriahyoung/Desktop/phyloseq_object/bacterial_2023_metadata.csv")
summarize_phyloseq(data_16S_unfiltered)

# filter out non bacteria
data_16S_uf1 <- subset_taxa(data_16S_unfiltered, Kingdom == "Bacteria" | Kingdom == "Archaea")
data_16S_uf2 <- subset_taxa(data_16S_uf1, Kingdom != "Eukaryota")
data_16S_uf3 <- subset_taxa(data_16S_uf2, Order != "Chloroplast")
data_16S_uf4 <- subset_taxa(data_16S_uf3, Family != "Mitochondria")

summarize_phyloseq(data_16S_uf4)

# Remove samples with extremely low read depth 
# data_16S_uf5 <- prune_samples(sample_sums(data_16S_uf4)>=1000, data_16S_uf4)

# Export filtered data to desktop AND REX google shared drive
# upload to desktop
# otu table
write.csv(data_16S_uf4@otu_table, "/Users/moriahyoung/Desktop/phyloseq_object/16S-2023-table-filtered.csv")
# taxonomy table
write.csv(data_16S_uf4@tax_table, "/Users/moriahyoung/Desktop/phyloseq_object/16S-2023-taxonomy-filtered.csv")

# upload these to REX google shared drive
# otu table
write.csv(data_16S_uf4@otu_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/2023/files to create phyloseq object/16S-2023-table-filtered.csv"), row.names=F)
# taxonomy table
write.csv(data_16S_uf4@tax_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/2023/files to create phyloseq object/16S-2023-taxonomy-filtered.csv"), row.names=F)
```

Removing contaminants
```{r}
# https://benjjneb.github.io/decontam/vignettes/decontam_intro.html
library(devtools)
library(processx)
#devtools::install_github("benjjneb/decontam", force = TRUE)
# Install decontam package which will be used to remove potential contaminants 
# by checking abundance distributions in negative controls vs regular samples
library(decontam) 
```

```{r}
# help from https://github.com/longleyr/Management-of-Soybean-Code-and-Files/blob/master/fungi_analysis_script.R

# rename phyloseq object name from above
phyloseq_soil <- data_16S_uf4

# check library size distribution
#write.csv(sample_data(phyloseq_soil), file = "sample_check1_soil.csv") # probably can delete
df_soil <- as.data.frame(sample_data(phyloseq_soil)) # Put sample_data into a ggplot-friendly data.frame
df_soil$LibrarySize_soil <- sample_sums(phyloseq_soil)
df_soil <- df_soil[order(df_soil$LibrarySize_soil),]
df_soil$Index <- seq(nrow(df_soil))
#write.csv(df_soil, file = "rank_sums_soil.csv") # probably can delete
ggplot(data=df_soil, aes(x=Index, y=LibrarySize_soil, color=Sample_or_Control)) + geom_point()

# filter by prevalence 
sample_data(phyloseq_soil)$is.neg <- sample_data(phyloseq_soil)$Sample_or_Control == "Control Sample"

contamdf.prev_soil <- isContaminant(phyloseq_soil, method="prevalence", neg="is.neg")
table(contamdf.prev_soil$contaminant)

# Make phyloseq object of presence-absence in negative controls and true samples
ps.pa_soil <- transform_sample_counts(phyloseq_soil, function(abund) 1*(abund>0))
ps.pa.neg_soil <- prune_samples(sample_data(ps.pa_soil)$Sample_or_Control == "Control Sample", ps.pa_soil)
ps.pa.pos_soil <- prune_samples(sample_data(ps.pa_soil)$Sample_or_Control == "True Sample", ps.pa_soil)

# Make data.frame of prevalence in positive and negative samples
df.pa_soil <- data.frame(pa.pos_soil=taxa_sums(ps.pa.pos_soil), pa.neg_soil=taxa_sums(ps.pa.neg_soil),
                          contaminant=contamdf.prev_soil$contaminant)
ggplot(data=df.pa_soil, aes(x=pa.neg_soil, y=pa.pos_soil, color=contaminant)) + geom_point() +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")

# remove contaminants
ps.noncontam_soil <- prune_taxa(!contamdf.prev_soil$contaminant, phyloseq_soil)
# with contaminants removed
otu_table(ps.noncontam_soil)

# remove controls
ps.noncontam_soil1 = subset_samples(ps.noncontam_soil, Sample_or_Control == "True Sample")

# Export the now filtered & non contaminated data to desktop AND the REX shared google drive
# to desktop
# otu table
write.csv(ps.noncontam_soil1@otu_table, "/Users/moriahyoung/Desktop/phyloseq_object/16S-2023-table-filtered-noncontam.csv")
# taxonomy table
write.csv(ps.noncontam_soil1@tax_table, "/Users/moriahyoung/Desktop/phyloseq_object/16S-2023-taxonomy-filtered-noncontam.csv")

# upload these to REX google shared drive
# update path
# otu table
write.csv(ps.noncontam_soil1@otu_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/2023/files to create phyloseq object/16S-2023-table-filtered-noncontam.csv"), row.names=F)
# taxonomy table
write.csv(ps.noncontam_soil1@tax_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/2023/files to create phyloseq object/16S-2023-taxonomy-filtered-noncontam.csv"), row.names=F)
```


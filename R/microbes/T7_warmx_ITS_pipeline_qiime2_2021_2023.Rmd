---
title: "REX T7 warmx ITS Sequencing"
author: "Moriah Young"
date: "2024-11-05"
output: pdf_document
---

After transferring data from GLOBUS from NCSU
```{bash}
# extract the contents of the FastqFiles_MS1_498_MYoungPLT-123_100270653.tar
# this command extracts the contents of the .tar and lists the files as they are being extracted
# -x: Extract files from the .tar
# -v: Verbose mode, which shows the progress of extraction by listing the files being extracted
# -f: Specifies the file name of the .tar (this must come last in the options list)
tar -xvf FastqFiles_MS1_498_MYoungPLT-123_100270653.tar

# move files to the "REX_ITS_2021_2023" folder in home directory
mv *.fastq.gz ~/REX_ITS_2021_2023/

# OR can do this in one line by telling it where to put the fastq files
tar -xvf FastqFiles_MS1_498_MYoungPLT-123_100270653.tar -C ~/REX_ITS_2021_2023
```

Making a mapping/manifest file
```{r}
library(tidyverse)
# FIRST STEPS/NOTE: NCSU did not give me a file that listed the fastq files they sent me (MSU's RTSF does this), which is helpful when making a manifest file SO, first, I accessed the MSU HPCC through the terminal and listed all the files in the folder I made for the fastq files. I copy and pasted those files into an excel sheet and saved as csv, which is the fastq data frame I read in below.

# create manifest dataframe
manifest <- data.frame (
        'sample-id' = character(226),
        'forward-absolute-filepath' = rep("/mnt/home/youngmor/REX_ITS_2021_2023/", 226),
        'reverse-absolute-filepath' = rep("/mnt/home/youngmor/REX_ITS_2021_2023/", 226),
        stringsAsFactors = FALSE
)

# read in .csv with fastq file names
fastq <- read.csv("/Users/moriahyoung/Desktop/Files to create REX ITS manifest file/REX_ITS_fastq_filenames.csv")

# Process fastq_df to extract sample_id and separate R1 and R2 files
fastq_processed <- fastq %>%
  mutate(
    # Extract sample_id as the characters before the first underscore
    sample_id = str_extract(fastq, "^[^_]+"),
    # Identify forward and reverse files
    forward_file = ifelse(str_detect(fastq, "_R1_"), str_extract(fastq, "[^/]+$"), NA),
    reverse_file = ifelse(str_detect(fastq, "_R2_"), str_extract(fastq, "[^/]+$"), NA)
  ) %>%
  # Group by sample_id so we have a single row per sample
  group_by(sample_id) %>%
  summarize(
    forward_file = na.omit(forward_file)[1],
    reverse_file = na.omit(reverse_file)[1],
    .groups = 'drop'
  )

# Merge `manifest` with `fastq_processed` on `sample_id` to populate columns
manifest <- manifest %>%
  # First fill in the sample_id column based on the position in fastq_processed
  mutate(sample_id = fastq_processed$sample_id[1:nrow(manifest)]) %>%
  # Append R1 and R2 filenames to paths in forward and reverse columns
  left_join(fastq_processed, by = "sample_id") %>%
  mutate(
    forward.absolute.filepath = ifelse(!is.na(forward_file),
                                       paste0(forward.absolute.filepath, forward_file),
                                       forward.absolute.filepath),
    reverse.absolute.filepath = ifelse(!is.na(reverse_file),
                                       paste0(reverse.absolute.filepath, reverse_file),
                                       reverse.absolute.filepath)
  ) %>%
  select(sample_id, forward.absolute.filepath, reverse.absolute.filepath)

# edit column names
names(manifest)[names(manifest) == "sample_id"] <- "sample-id"
names(manifest)[names(manifest) == "forward.absolute.filepath"] <- "forward-absolute-filepath"
names(manifest)[names(manifest) == "reverse.absolute.filepath"] <- "reverse-absolute-filepath"

# delete line #64
manifest <- manifest[-64,]

# Write the data frame to a .txt file
# this file needs to be uploaded to the folder that has your sequences in on the HPCC so that it can we used further down in the pipeline with qiime2
write.table(manifest, "/Users/moriahyoung/Desktop/Files to create REX ITS manifest file/REX_ITS_2021_2023_manifest.txt", sep = "\t", row.names = FALSE, quote = FALSE)
```

```{bash}
#load anaconda
module load Conda/3

#activate qiime2
conda activate qiime2-2022.11

# random notes
# Ctrl + C to stop a process running on a terminal
#SBATCH --output=~/%x-%j.slurmout # x is job name and j is job number and the outputs will go here
```

Import demultiplexed sequences into .qza artifact for downstream pipeline using Qiime2
```{bash}
cd /mnt/home/youngmor/REX_ITS_2021_2023
qiime tools import 
--type 'SampleData[PairedEndSequencesWithQuality]' \
--input-path REX_ITS_2021_2023_manifest.txt \
--input-format PairedEndFastqManifestPhred33V2 \
--output-path REX_ITS_2021_2023.qza \

qiime tools import --type 'SampleData[PairedEndSequencesWithQuality]' --input-path REX_ITS_2021_2023_manifest.txt --input-format PairedEndFastqManifestPhred33V2 --output-path REX_ITS_2021_2023.qza
```

Evaluate data quality
We need to examine the sequence quality to determine how we should trim the sequences. If you have run FastQC on your raw sequence files, you can examine the per base sequence quality category. Qiime also provides a plugin for graphing sequence quality. Add the input and output names to the following commands and run:
```{bash}
cd /mnt/home/youngmor/REX_ITS_2021_2023
qiime demux summarize \
--i-data REX_ITS_2021_2023.qza \
--o-visualization REX_ITS_2021_2023_SUMMARY_VIZ.qzv
qiime tools view REX_ITS_2021_2023_SUMMARY_VIZ.qzv

qiime demux summarize --i-data REX_ITS_2021_2023.qza --o-visualization REX_ITS_2021_2023_SUMMARY_VIZ.qzv
```

Denoise sequences using dada2
The qiime dada2 denoise-paired command runs the DADA2 pipeline on paired-end sequence data in QIIME 2. DADA2 is a popular method for quality filtering, trimming, merging, and denoising high-throughput sequencing reads.
```{bash}
cd /mnt/home/youngmor/REX_ITS_2021_2023
qiime dada2 denoise-paired --i-demultiplexed-seqs REX_ITS_2021_2023.qza \
--p-trim-left-f 0 \
--p-trim-left-r 0 \
--p-trunc-len-f 250 \ # this number will change based on the quality of your sequences
--p-trunc-len-r 250 \ # this number will change based on the quality of your sequences
--o-table REX_ITS_2021_2023-dada2table.qza \
--o-representative-sequences REX_ITS_2021_2023-rep-seqs.qza \
--o-denoising-stats REX_ITS_2021_2023-denoising-stats.qza \
--p-n-threads 36

#job below on hpcc took 03:55:40
#!/bin/bash --login
########## SBATCH Lines for Resource Request ##########
 
#SBATCH --time=24:00:00             # limit of wall clock time - how long the job will run (same as -t)
#SBATCH --ntasks=16
#SBATCH --mem-per-cpu=64G            # memory required per allocated CPU (or core) - amount of memory (in bytes)
#SBATCH --job-name qiime_denoising5      # you can give your job a name for easier identification (same as -J)
#SBATCH --mail-type=ALL
#SBATCH --mail-user=youngmor@msu.edu 

qiime dada2 denoise-paired --i-demultiplexed-seqs REX_ITS_2021_2023.qza --p-trim-left-f 0 --p-trim-left-r 0 --p-trunc-len-f 250 --p-trunc-len-r 250 --o-table REX_ITS_2021_2023-dada2table.qza --o-representative-sequences REX_ITS_2021_2023-rep-seqs.qza --o-denoising-stats REX_ITS_2021_2023-denoising-stats.qza --p-n-threads 36 --verbose
```

Create a .qzv file so you can check out the table from the denoising stats that was created from the job above
You can then drag and drop the .qzv file into this website: https://view.qiime2.org/ 
You'll need to download it from the interactive interface onto your mac
```{bash}
# Run 1
cd /mnt/home/youngmor/REX_ITS_2021_2023
qiime metadata tabulate \
  --m-input-file REX_ITS_2021_2023-denoising-stats.qza \
  --o-visualization REX_ITS_2021_2023-denoising-stats.qzv
  
qiime metadata tabulate --m-input-file REX_ITS_2021_2023-denoising-stats.qza --o-visualization REX_ITS_2021_2023-denoising-stats.qzv
```

# Test the classifier
# below I'm using a pre-trained classifier from: https://github.com/colinbrislawn/unite-train/releases but I can also train my own classifier which I might want to do
```{bash}
cd /mnt/home/youngmor/REX_ITS_2021_2023
qiime feature-classifier classify-sklearn \
--i-classifier unite_ver9_dynamic_29.11.2022-Q2-2022.11.qza \
--i-reads REX_ITS_2021_2023-rep-seqs.qza \
--o-classification REX_ITS_2021_2023-taxonomy.qza

qiime feature-classifier classify-sklearn --i-classifier unite_ver9_dynamic_29.11.2022-Q2-2022.11.qza --i-reads REX_ITS_2021_2023-rep-seqs.qza --o-classification REX_ITS_2021_2023-taxonomy.qza

# visualize
qiime metadata tabulate \
  --m-input-file REX_ITS_2021_2023-taxonomy.qza \
  --o-visualization REX_ITS_2021_2023-taxonomy.qzv

# testing the classifier and creating the .qzv file took 7 hours on the hpcc (using the same SLURM code/settings as the denoising script above)
  
qiime metadata tabulate --m-input-file REX_ITS_2021_2023-taxonomy.qza --o-visualization REX_ITS_2021_2023-taxonomy.qzv
```

# Generate a phylogenetic tree
https://docs.qiime2.org/2023.2/tutorials/phylogeny/#root-the-phylogeny
```{bash}
cd /mnt/home/youngmor/REX_ITS_2021_2023

# this aligns your sequences
qiime alignment mafft \
  --i-sequences REX_ITS_2021_2023-rep-seqs.qza \
  --o-alignment REX_ITS_2021_2023-aligned-rep-seqs.qza
  
qiime alignment mafft REX_ITS_2021_2023-rep-seqs.qza --o-alignment REX_ITS_2021_2023-aligned-rep-seqs.qza
  
qiime alignment mask \
  --i-alignment REX_ITS_2021_2023-aligned-rep-seqs.qz \
  --o-masked-alignment REX_ITS_2021_2023-masked-aligned-rep-seqs.qza
  
qiime alignment mask --i-alignment REX_ITS_2021_2023-aligned-rep-seqs.qza --o-masked-alignment REX_ITS_2021_2023-masked-aligned-rep-seqs.qza

# make the tree
# using fasttree
qiime phylogeny fasttree \
  --i-alignment REX_ITS_2021_2023-masked-aligned-rep-seqs.qza \
  --o-tree REX_ITS_2021_2023-fasttree-unrooted-tree.qza
  --verbose
  
qiime phylogeny fasttree --i-alignment REX_ITS_2021_2023-masked-aligned-rep-seqs.qza --o-tree REX_ITS_2021_2023-fasttree-unrooted-tree.qza --verbose

#root the tree using the longest root
qiime phylogeny midpoint-root \
  --i-tree REX_ITS_2021_2023-fasttree-unrooted-tree.qza \
  --o-rooted-tree REX_ITS_2021_2023-fasttree-rooted-tree.qza

qiime phylogeny midpoint-root --i-tree REX_ITS_2021_2023-fasttree-unrooted-tree.qza --o-rooted-tree REX_ITS_2021_2023-fasttree-rooted-tree.qza
```

# create mapping/metadata file
```{r}
# Clear all existing data
rm(list=ls())

# Set working directory from .Renviron
dir <- Sys.getenv("DATA_DIR")
list.files(dir)

library(tidyverse)

# read in data
microbe_metadata_2021 <- read.csv(file.path(dir, "microbes/16S & ITS/T7_warmx/L0/ITS/files to create REX ITS metadata/MY_REX_soil_microbes_2021_metadata.csv"))
microbe_metadata_2023 <- read.csv(file.path(dir, "microbes/16S & ITS/T7_warmx/L0/ITS/files to create REX ITS metadata/MY_REX_soil_microbes_2023_metadata.csv"))
warmx_metadata <- read.csv(file.path(dir, "microbes/16S & ITS/T7_warmx/L0/ITS/files to create REX ITS metadata/REX_warmx_metadata.csv"))

# add year column to microbe metadata dataframes
microbe_metadata_2021$Year <- 2021
microbe_metadata_2023$Year <- 2023

# merge files above
its_metadata <- full_join(microbe_metadata_2021, microbe_metadata_2023)

its_metadata_1 <- full_join(its_metadata, warmx_metadata, by = c("Unique_ID", "Treatment", "Replicate",
                                      "Footprint", "Footprint_Location", "Subplot", "Subplot_Location"))

# delete columns
# Unique_Field_Location_Code
its_metadata_1 <- subset(its_metadata_1, select = -c(Unique_Field_Location_Code))

# reorder columns
its_metadata_1 <- its_metadata_1[, c("Year", "SampleID", "Treatment", "Replicate", "Rep", "Footprint", "Footprint_Location", "Rep_Footprint", "Footprint_Treatment_full","Subplot", "Subplot_Location", "Drought", "Warming", "Insecticide", "Subplot_Descriptions", "Unique_ID", "sampling_period")]

# Replace underscores with dashes in the SampleID column
its_metadata_1$SampleID <- gsub("_", "-", its_metadata_1$SampleID)

# write a new csv with the cleaned and merge data 
write.csv(its_metadata_1, file = "/Users/moriahyoung/Desktop/phyloseq_object/ITS_2021_2023_metadata.csv", row.names = F)
```

Cleaning and steps to make a phyloseq object
```{r}
library(qiime2R)
# Read QZA files into dataframe, re-format taxonomic tables, and re-upload them as .csv files
# Code below you only need to do once
# download the qza files from the HPCC
SVsITS <- read_qza("/Users/moriahyoung/Downloads/REX_ITS_2021_2023-dada2table.qza")
SVsITStable <- SVsITS$data
write.csv(SVsITStable, file = "/Users/moriahyoung/Downloads/REX_ITS_2021_2023-dada2table.csv")

taxonomyITS <- read_qza("/Users/moriahyoung/Downloads/REX_ITS_2021_2023-taxonomy.qza")
taxITS <- taxonomyITS$data %>% as_tibble() %>%
  mutate(Taxon=gsub("[a-z]__", "", Taxon)) %>% 
  separate(Taxon, sep=";", c("Kingdom","Phylum","Class","Order","Family","Genus","Species"))%>%
  mutate(Phylum=replace_na(Phylum,"empty"))
write.csv(taxITS, file = "/Users/moriahyoung/Downloads/REX_ITS_2021_2023-taxonomy.csv", row.names =F)
```

Filter data
```{r}
library(phyloseq)
library(microbiome)

# create phyloseq object
data_its_unfiltered <- read_csv2phyloseq(otu.file = "/Users/moriahyoung/Downloads/REX_ITS_2021_2023-dada2table.csv",
                                         taxonomy.file = "/Users/moriahyoung/Downloads/REX_ITS_2021_2023-taxonomy.csv",
                                         metadata.file = "/Users/moriahyoung/Desktop/phyloseq_object/ITS_2021_2023_metadata.csv")
summarize_phyloseq(data_its_unfiltered)

# filter out non bacteria
data_its_uf1 <- subset_taxa(data_its_unfiltered, Kingdom == "Fungi")
data_its_uf2 <- subset_taxa(data_its_uf1, Kingdom != "Bacteria" | Kingdom != "Archaea")
data_its_uf3 <- subset_taxa(data_its_uf2, Order != "Chloroplast")
data_its_uf4 <- subset_taxa(data_its_uf3, Family != "Mitochondria")

summarize_phyloseq(data_its_uf4)

# Remove samples with extremely low read depth 
# data_16S_uf5 <- prune_samples(sample_sums(data_16S_uf4)>=1000, data_16S_uf4)

# Export filtered data to desktop AND REX google shared drive
# upload to desktop
# otu table
write.csv(data_its_uf4@otu_table, "/Users/moriahyoung/Desktop/phyloseq_object/REX_ITS_2021_2023-table-filtered.csv")
# taxonomy table
write.csv(data_its_uf4@tax_table, "/Users/moriahyoung/Desktop/phyloseq_object/REX_ITS_2021_2023-taxonomy-filtered.csv")

# upload these to REX google shared drive
# otu table
write.csv(data_its_uf4@otu_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/ITS/files to create phyloseq object/REX_ITS_2021_2023-table-filtered.csv"), row.names=F)
# taxonomy table
write.csv(data_its_uf4@tax_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/ITS/files to create phyloseq object/REX_ITS_2021_2023-taxonomy-filtered.csv"), row.names=F)
```

Removing contaminants
```{r}
# https://benjjneb.github.io/decontam/vignettes/decontam_intro.html
library(devtools)
library(processx)
#devtools::install_github("benjjneb/decontam", force = TRUE)
# Install decontam package which will be used to remove potential contaminants 
# by checking abundance distributions in negative controls vs regular samples
library(decontam) 
```

```{r}
# help from https://github.com/longleyr/Management-of-Soybean-Code-and-Files/blob/master/fungi_analysis_script.R

# rename phyloseq object name from above
phyloseq_soil <- data_its_uf4

# check library size distribution
#write.csv(sample_data(phyloseq_soil), file = "sample_check1_soil.csv") # probably can delete
df_soil <- as.data.frame(sample_data(phyloseq_soil)) # Put sample_data into a ggplot-friendly data.frame
df_soil$LibrarySize_soil <- sample_sums(phyloseq_soil)
df_soil <- df_soil[order(df_soil$LibrarySize_soil),]
df_soil$Index <- seq(nrow(df_soil))
#write.csv(df_soil, file = "rank_sums_soil.csv") # probably can delete
ggplot(data=df_soil, aes(x=Index, y=LibrarySize_soil, color=Sample_or_Control)) + geom_point()

# filter by prevalence 
sample_data(phyloseq_soil)$is.neg <- sample_data(phyloseq_soil)$Sample_or_Control == "Control Sample"

contamdf.prev_soil <- isContaminant(phyloseq_soil, method="prevalence", neg="is.neg")
table(contamdf.prev_soil$contaminant)

# Make phyloseq object of presence-absence in negative controls and true samples
ps.pa_soil <- transform_sample_counts(phyloseq_soil, function(abund) 1*(abund>0))
ps.pa.neg_soil <- prune_samples(sample_data(ps.pa_soil)$Sample_or_Control == "Control Sample", ps.pa_soil)
ps.pa.pos_soil <- prune_samples(sample_data(ps.pa_soil)$Sample_or_Control == "True Sample", ps.pa_soil)

# Make data.frame of prevalence in positive and negative samples
df.pa_soil <- data.frame(pa.pos_soil=taxa_sums(ps.pa.pos_soil), pa.neg_soil=taxa_sums(ps.pa.neg_soil),
                          contaminant=contamdf.prev_soil$contaminant)
ggplot(data=df.pa_soil, aes(x=pa.neg_soil, y=pa.pos_soil, color=contaminant)) + geom_point() +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")

# remove contaminants
ps.noncontam_soil <- prune_taxa(!contamdf.prev_soil$contaminant, phyloseq_soil)
# with contaminants removed
otu_table(ps.noncontam_soil)

# remove controls
ps.noncontam_soil1 = subset_samples(ps.noncontam_soil, Sample_or_Control == "True Sample")

# Export the now filtered & non contaminated data to desktop AND the REX shared google drive
# to desktop
# otu table
write.csv(ps.noncontam_soil1@otu_table, "/Users/moriahyoung/Desktop/phyloseq_object/its-table-filtered-noncontam.csv")
# taxonomy table
write.csv(ps.noncontam_soil1@tax_table, "/Users/moriahyoung/Desktop/phyloseq_object/its-taxonomy-filtered-noncontam.csv")

# upload these to REX google shared drive
# update path
# otu table
write.csv(ps.noncontam_soil1@otu_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/ITS/files to create phyloseq object/its-table-filtered-noncontam.csv"), row.names=F)
# taxonomy table
write.csv(ps.noncontam_soil1@tax_table, file.path(dir,"microbes/16S & ITS/T7_warmx/L0/ITS/files to create phyloseq object/its-taxonomy-filtered-noncontam.csv"), row.names=F)
```

